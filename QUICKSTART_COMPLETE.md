# ğŸš€ GARE EASY - Quick Start Guide

## âœ… Setup Complete!

Your Gare Easy system is ready with:
- âœ… Database layer with CRUD operations
- âœ… MEF scraper with real selectors
- âœ… Document downloader for attachments
- âœ… PDF text extraction (pdfplumber)
- âœ… Google Gemini AI for Level 2 data extraction
- âœ… Streamlit dashboard with filters and analytics
- âœ… APScheduler for 6-hour automatic updates

---

## ğŸ“‹ How to Use

### Option 1: Run Single Scrape (Test)
```bash
# Windows
run_scraper_once.bat

# Or manually
python main.py --platform mef --mode once
```

This will:
1. Scrape MEF platform for tenders
2. Download attachments
3. Extract text from PDFs
4. Use Gemini AI to extract Level 2 data
5. Store everything in database

### Option 2: Run Scheduler (Production)
```bash
# Windows
run_scheduler.bat

# Or manually
python main.py --mode schedule --platform mef
```

This will:
- Run scraper every 6 hours automatically
- Keep updating tenders and checking for changes
- Run in background until you press Ctrl+C

### Option 3: View Dashboard
```bash
# Windows
run_dashboard.bat

# Or manually
streamlit run streamlit_app/app.py
```

Open browser to: http://localhost:8501

Dashboard features:
- ğŸ“Š Overview with key metrics
- ğŸ“‹ Searchable tender table
- ğŸ“ˆ Analytics and charts
- ğŸ” Detailed tender view with Level 2 data

---

## ğŸ”§ Configuration

### Google API Key (Already Set)
Your key is in `.env`:
```
GOOGLE_API_KEY=AIzaSyDWcNEJrB-hoMeU5Fgc8nh3U5sXxlyqbqw
```

### Update Frequency
Edit `config/config.yaml`:
```yaml
scraper:
  update_interval_hours: 6  # Change to 3, 12, 24, etc.
```

### Enable/Disable AI
```yaml
level2:
  enabled: true  # Set to false to skip AI extraction
  model: gemini-pro  # Free tier model
```

---

## ğŸ“ Project Structure

```
gare_easy/
â”œâ”€â”€ main.py                  # Main entry point
â”œâ”€â”€ run_dashboard.bat        # Quick start dashboard
â”œâ”€â”€ run_scheduler.bat        # Quick start scheduler
â”œâ”€â”€ run_scraper_once.bat     # Quick start single run
â”œâ”€â”€ config/
â”‚   â””â”€â”€ config.yaml          # Configuration
â”œâ”€â”€ database/
â”‚   â”œâ”€â”€ db_manager.py        # CRUD operations
â”‚   â””â”€â”€ models.py            # SQLAlchemy models
â”œâ”€â”€ scrapers/
â”‚   â”œâ”€â”€ base_scraper.py      # Base scraper class
â”‚   â””â”€â”€ mef_scraper.py       # MEF platform scraper
â”œâ”€â”€ processors/
â”‚   â”œâ”€â”€ document_processor.py # Download attachments
â”‚   â”œâ”€â”€ pdf_extractor.py     # Extract text from PDFs
â”‚   â””â”€â”€ ai_processor.py      # Gemini AI integration
â”œâ”€â”€ scheduler/
â”‚   â””â”€â”€ job_scheduler.py     # APScheduler wrapper
â”œâ”€â”€ streamlit_app/
â”‚   â””â”€â”€ app.py               # Dashboard UI
â””â”€â”€ data/
    â”œâ”€â”€ gare_easy.db         # SQLite database
    â””â”€â”€ downloads/           # Downloaded PDFs
```

---

## ğŸ¯ Scoring Status

### Implemented (85+ points)
- âœ… **MEF Platform** (50 points) - Complete with real selectors
- âœ… **Database Layer** (10 points) - CRUD + upsert + change detection
- âœ… **Document Downloader** (10 points) - Download attachments to organized folders
- âœ… **Level 2 Extraction** (15 points) - Gemini AI for qualifications, criteria, etc.
- âœ… **Streamlit Dashboard** (10 points) - Filters, metrics, analytics, detail view
- âœ… **Scheduler** (10 points) - 6-hour automatic updates

### Ready to Add (100+ points)
- â³ **Regional Platforms** (20 points each)
  - Aria, Toscana, Empulia, Emilia, ASMeComm
  - Copy MEF scraper pattern, update selectors

---

## ğŸ§ª Testing

### Test Database Layer
```bash
python test_database.py
```

### Test Document Processor
```bash
python test_document_processor.py
```

### Test AI Processor
```bash
python processors/ai_processor.py
```

---

## ğŸ“Š Database Schema

### Main Tables
- **tenders** - Level 1 data (title, amount, deadline, etc.)
- **level2_data** - AI-extracted data (qualifications, criteria, etc.)
- **attachments** - File metadata and download status
- **scraper_logs** - Execution history

### Key Queries
```python
# Get tenders without Level 2 data
tenders = db.get_tenders_without_level2('MEF', limit=10)

# Get undownloaded attachments
attachments = db.get_undownloaded_attachments(tender_id)

# Get statistics
stats = db.get_statistics()
```

---

## ğŸ› Troubleshooting

### "No tenders in database"
Run the scraper first:
```bash
python main.py --platform mef --mode once
```

### "AI extraction failed"
Check API key:
```bash
python -c "import os; from dotenv import load_dotenv; load_dotenv(); print(os.getenv('GOOGLE_API_KEY'))"
```

### "Module not found"
Install dependencies:
```bash
pip install -r requirements.txt
```

### Playwright browser issues
Install browsers:
```bash
playwright install chromium
```

---

## ğŸ“ Next Steps

1. **Run first scrape**: `run_scraper_once.bat`
2. **Check dashboard**: `run_dashboard.bat`
3. **Start scheduler**: `run_scheduler.bat` (for production)
4. **Add more platforms**: Copy `mef_scraper.py`, update selectors

---

## ğŸ“ Support

- Check logs in `logs/scraper.log`
- Review database with SQLite browser
- Test individual components with test files

**Ready to go! Run `run_scraper_once.bat` to start.**
